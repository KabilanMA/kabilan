---
layout: post
title: Hello World in MLIR
date: 2026-02-12 06:01:00
description: Start writing with MLIR
tags: formatting code
categories: notes
thumbnail: assets/img/blogs/mlir.png
giscus_comments: true
related_posts: false
toc:
  beginning: false

---

## Introduction
---

MLIR is not a programming language like C++ or Python; it is a compiler infrastructure—a framework of tools rather than a single tool. Because of this, there is no single "Hello World" program. As I’ve been learning, I’ve realized there are actually three different ways to use this framework, and you need to understand all of them.

1. Write MLIR program directly using existing dialects.
2. Create a MLIR pass for either optimization of analysis.
3. Build our own dialects by extending the language itself by defining new operations and types.

To really understand how MLIR works, I’m going to walk through a minimal "Hello World" for all three scenarios.

### Write MLIR Program

The MLIR distribution includes a rich set of in-tree dialects, most of which target high-performance tensor compilers and hardware-specific code generation. However, before we tackle those, we need to understand the basics of the IR itself. We will start by manually writing a program using two core dialects: `func` (for function abstraction) and `arith` (for basic arithmetic operations). This will allow us to see how MLIR represents logic and how the infrastructure processes it without getting bogged down in complex types.

Our goal is to get from high-level abstractions down to executable code. To do that, we’ll take a high-level MLIR program (using `func` and `arith`), lower it into MLIR's LLVM Dialect, which is basically a 1:1 mapping of LLVM instructions inside MLIR, and finally translate that into pure LLVM IR (`.ll`) so the machine can actually run it. Our MLIR program should define a function which takes two integer (`i32`) arguments and add them together and return the results.

In MLIR, everything is an operation. Operations are the core unit of abstraction and computation, similar in many ways to LLVM instructions. Even defining a function is an operation! The `func` dialect has an operation called `func` to define functions. We use the `@` sigil for global symbols (like function names) and the `%` sigil for local values (variables).
```llvm
func.func @add(%arg0: i32, %arg1: i32) -> i32 {
```
This tells the compiler that we are defining a function called `@add` which takes two `i32` arguments `%arg0` and `%arg1` and returns a `i32` value.

Now let's take a look at the logic inside the block.

```llvm
func.func @add(%arg0: i32, %arg1: i32) -> i32 {
    %0 = arith.addi %arg0, %arg1 : i32
    func.return %0 : i32
}
```

Here, `arith.addi` does the heavy lifting. It grabs `%arg0` and `%arg1`, adds them up, and assigns the result to `%0`. Since every function block needs to end explicitly, we then use the `func.return` operation to send that `%0` value back out.

Go ahead and save this code in a file called `add.mlir`. Next, we are going to use the `mlir-opt` tool to lower our high-level dialects (`func` and `arith`) down to the LLVM Dialect. If that sounds confusing, don't worry, it tripped me up at first, too. You might be wondering: Why are we translating to an 'LLVM dialect' inside MLIR instead of just spitting out actual LLVM IR? > That question actually hits on the entire purpose of MLIR. By keeping LLVM instructions represented as an MLIR dialect, the framework can preserve high-level structure and debug information during the translation process. The ultimate goal for almost any MLIR pipeline is to gradually step down the abstraction ladder until you hit this LLVM dialect. Once you are there, escaping out to standard LLVM IR is trivial.

```bash
mlir-opt add.mlir --convert-arith-to-llvm --convert-func-to-llvm > add.llvm.mlir
```

So, what is happening with this command?

```
func  ----
          \____ llvm
arith ----/
```

As mentioned earlier, this process is called __Lowering__, where we are taking a MLIR program written using `func` and `arith` dialects and lower them into a single, lower-level dialect-`llvm`. You can use `mlir-opt --help` to look into all other possible `mlir-opt` flags.

Now, ensure that you have the file `add.llvm.mlir`, which contains the translated code in the LLVM dialect as follows:
```llvm
module {
  llvm.func @add(%arg0: i32, %arg1: i32) -> i32 {
    %0 = llvm.add %arg0, %arg1 : i32
    llvm.return %0 : i32
  }
}
```
At first glance, it looks almost identical. But look closely at the prefixes. Every operation has shifted from a high-level abstraction to a specific LLVM instruction representation and now explictly wrapped in a `module`. In MLIR, operations cannot just float in the coid; they must live inside a block. Top-level operations like functions needs a container. The `module` operation acts as that container. Even though we did not explictly define a module ourselves, a `module` is implicit in MLIR programs which allows you to write top-level functions for convenience, but internally, it treats them as if they are inside a module. `module` defines a symbol table and creates a scope where global names (like `@add`) are defined and unique. This is roughly corresposnding to a translation unit in C++ or an object file. Our current program is entirly written using only one dialect-LLVM dialect and translated using `mlir-opt` as above.

We are almost there. We have lowered out high-level logic into the low-level LLVM dialect. But here is the catch: we still cannot execute this. The machine does not know what MLIR is. It doesn't know about dialects and operations. To run this, we need to leave the MLIR framework entirely and generate actual __LLVM IR__, the standard text-based format (`.ll`) that the LLVM backend understands.

MLIR framework has a tool specifically for this purpose, `mlir-translate`. While `mlir-opt` is for transforming code within MLIR (optimization, lowering), `mlir-translate` is for exporting code out of MLIR. We will use `mlir-translate` to take out LLVM-dialect module and serialize it into valid LLVM IR using the following command:
```bash
mlir-translate --mlir-to-llvmir add.llvm.mlir -o add.ll
```
If you open `add.ll`, you will see something familiar to any compiler engineer: standard, raw LLVM IR. We now have `add.ll`, which contains our `@add` function in valid LLVM IR. But if you try to compile it directly, nothing will happen. Why? Because we don't have a `main` function. Every C/C++ (and LLVM) programs needs an entry point. Our `@add` function is just a library function right now; it's waiting to be called, but nobody is calling it. Therefore, let's create a simple driver program to fix this problem.

driver.ll
```llvm
; Declare the external 'add' function (it exists in our other file)
declare i32 @add(i32, i32)

define i32 @main() {
  ; Call 'add' with arguments 10 and 32
  %result = call i32 @add(i32 10, i32 32)
  
  ; Return the result as the exit code
  ret i32 %result
}
```

This is raw LLVM IR. It tells the compiler: "There is a function called `@add` somewhere else. I want to call it with `10` and `32`, and I want to return the answer as my program's exit code." Now that we have the `main` function too, let's use `clang` to link them together into a real executable binary and run it.
```bash
clang add.ll driver.ll -o add
./add
```
But wait!, where is the output? If you run it, you won't see anything. That's expected! We didn't tell the program to print anything (which requires library calls like `printf`). We told it to return the results as the exit code. To see the exit code in your terminal, run this:
```bash
./add ; echo $?
```
If everything worked, you should see: `42`

Success! We did it. We started with high-level MLIR (func + arith), lowered it to the LLVM dialect, translated it to LLVM IR, linked it with a driver, and executed it on the bare metal.

You have officially written, compiled, and run your first MLIR program.

### Create MLIR Pass

This is where things get real. Writing MLIR code (`.mlir`) is like writing Python, you are a user. Writing a Pass means writing C++, you are now a compiler engineer. In this section, we aren't just running `mlir-opt`; we are building our own version of it. If you have used LLVM before, the traditional workflow: compile your pass into a shared library (`.so`) and then load it dynamically into the standard `opt` tool using a flag like `-load-pass-plugin`. In MLIR, we rarely do that. Instead, we almost always build our own custom version of `mlir-opt`. 

Why? Because, MLIR relies heavily on C++ templates and static registration. Because of how C++ templates work, they don't play nice across library boundaries, trying to load an MLIR pass dynamically can lead to weird ABI issues or missing symbols. So the "MLIR Way" is to create a new C++ executable that links statically against the core MLIR libraries, the standard dialect libraries and our custom pass library. This results in a tool that functions exactly like `mlir-opt`, but extended to specifically include your pass.

Let's create a new directory for our "Hello World" MLIR pass called `PrintOpsPass` which can simply print the names of each operations. If you look at the C++ code for a pass, it can be intimidating. There are templates nested inside templates and macros with names that look like they were shouted by a compiler. But once you strip away the noise, it's actually quite elegant.

Create a file called `my-opt.cpp`, and start writing the pass by wrapping everything in an anonymous namespace (`namespace {...}`). In C++, this is a trick to make sure our class is only visible inside this specific file. We do this because eventually, we might link dozens of passes together into one tool, and we don't want out `PrintOpsPass` colliding with someone else's `PrintOpsPass`.

```cpp
using namespace mlir;

namespace {
      struct PrintOpsPass : PassWrapper<PrintOpsPass, Operation<ModuleOp>> {}
}
```
The above code defines our pass but at first glance it looks scary, especially, if you have not used [CRTP](https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern)(Curiously Recurring Template Pattern) in C++ before. Let's look into it to clear away all the noise. We are defining our pass by inheriting from `PassWrapper`. This is a helper class MLIR provides to handle all the boilerplate of registering a pass. Also, notice the template arguments: `PrintOpsPass` and `OperationPass<ModuleOp>`. We are defining struct `PrintOpsPass`, but we are passing `PrintOpsPass` as a template argument to its own parent! This is known as the __CRTP__.

In standard C++ OOP, runtime polymorphism is typically implemented using virtual functions, which introduce indirection via vtable lookups. While this overhead is negligible in most applications, it can be significant in compiler hot paths that execute at scale. Therefore we are passing `PrintOpsPass` into `PassWrapper` to give the parent class a compile-time pointer to its child. This allows `PassWrapper` to implement boilerplate methods (like `clone()` or `getId()`) for us, specifically customized for our class, without needing a single virtual function call. This is called static polymorphism.

